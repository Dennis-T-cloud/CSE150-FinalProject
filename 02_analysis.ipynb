{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80250857",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 02_analysis.ipynb \n",
    "# Imports and basic setup for analysis\n",
    "\"\"\"\n",
    "02_analysis.ipynb builds on top of that pipeline and focuses on evaluation and interpretation: \n",
    "    regime stability across seeds and samples, macroeconomic alignment, and simple ROC-style analysis \n",
    "    for downturn detection.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from hmm_utils import (     # from mini-project style HMM\n",
    "    baum_welch_train,      \n",
    "    viterbi_decode,        \n",
    "    forward_pass,          \n",
    "    backward_pass,        \n",
    "    compute_posteriors,   \n",
    "    count_num_params,\n",
    "    compute_aic_bic,\n",
    ")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "DATA_PATH = Path(\"data\") / \"fhfa_hpi_raw.csv\"   # adjust to your actual file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005eca42",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load and preprocess HPI data (same idea as in final_pipeline)\n",
    "\n",
    "def load_and_preprocess_hpi(filepath):\n",
    "    \"\"\"\n",
    "    Load FHFA HPI data and build a clean time series with log returns.\n",
    "\n",
    "    Feel free to copy the exact version from final_pipeline.ipynb.\n",
    "    Here we assume:\n",
    "      - there is a 'date' column\n",
    "      - there is an 'hpi' column with the index level\n",
    "    \"\"\"\n",
    "    df_raw = pd.read_csv(filepath)\n",
    "\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # parse date if present\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        df = df.sort_values(\"date\")\n",
    "    else:\n",
    "        df = df.sort_index()\n",
    "\n",
    "    if \"hpi\" not in df.columns:\n",
    "        raise ValueError(\"Please rename your HPI level column to 'hpi' or update this loader.\")\n",
    "\n",
    "    df[\"log_hpi\"] = np.log(df[\"hpi\"])\n",
    "    df[\"ret\"] = df[\"log_hpi\"].diff()\n",
    "\n",
    "    df = df.dropna(subset=[\"ret\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_and_preprocess_hpi(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd23eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Wrapper to train HMM for a single K and random seed\n",
    "\n",
    "def train_hmm_with_seed(observations, K, max_iters=100, n_restarts=3, seed=0):\n",
    "    \"\"\"\n",
    "    Train a Gaussian HMM with K states for a given random seed.\n",
    "\n",
    "    This is basically the same pattern as in final_pipeline.\n",
    "    It uses baum_welch_train and viterbi_decode from the mini-project style code.\n",
    "    \"\"\"\n",
    "    T = len(observations)\n",
    "\n",
    "    best_params, loglik_trace = baum_welch_train(\n",
    "        observations,\n",
    "        K,\n",
    "        max_iters=max_iters,\n",
    "        tol=1e-6,\n",
    "        n_restarts=n_restarts,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    final_loglik = loglik_trace[-1]\n",
    "    num_params = count_num_params(K)\n",
    "    aic, bic = compute_aic_bic(final_loglik, num_params, T)\n",
    "    viterbi_path = viterbi_decode(observations, best_params)\n",
    "\n",
    "    result = {\n",
    "        \"K\": K,\n",
    "        \"params\": best_params,\n",
    "        \"loglik_trace\": loglik_trace,\n",
    "        \"final_loglik\": final_loglik,\n",
    "        \"AIC\": aic,\n",
    "        \"BIC\": bic,\n",
    "        \"viterbi_path\": viterbi_path,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae88fd11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Regime stability across multiple random seeds for a fixed K (for example, K = 3)\n",
    "\n",
    "observations = df[\"ret\"].values\n",
    "\n",
    "K_target = 3\n",
    "seeds = [0, 1, 2, 3, 4]   # you can extend this if you want\n",
    "\n",
    "results_multi_seed = []\n",
    "\n",
    "for s in seeds:\n",
    "    print(f\"Training HMM with K={K_target}, seed={s}...\")\n",
    "    res = train_hmm_with_seed(observations, K_target, max_iters=100, n_restarts=2, seed=s)\n",
    "    results_multi_seed.append(res)\n",
    "\n",
    "print(\"Done training multiple seeds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f133b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Compute pairwise similarity of Viterbi paths (Hamming-based agreement)\n",
    "\n",
    "def path_agreement(path1, path2):\n",
    "    \"\"\"\n",
    "    Simple Hamming-style agreement between two integer paths.\n",
    "\n",
    "    Note: This does not try to fix label switching.\n",
    "    For this course project, a rough measure is fine.\n",
    "    \"\"\"\n",
    "    assert len(path1) == len(path2)\n",
    "    return np.mean(path1 == path2)\n",
    "\n",
    "\n",
    "n_runs = len(results_multi_seed)\n",
    "agreement_matrix = np.zeros((n_runs, n_runs))\n",
    "\n",
    "for i, j in combinations(range(n_runs), 2):\n",
    "    p_i = results_multi_seed[i][\"viterbi_path\"]\n",
    "    p_j = results_multi_seed[j][\"viterbi_path\"]\n",
    "    score = path_agreement(p_i, p_j)\n",
    "    agreement_matrix[i, j] = score\n",
    "    agreement_matrix[j, i] = score\n",
    "\n",
    "np.fill_diagonal(agreement_matrix, 1.0)\n",
    "\n",
    "agreement_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36858c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot a heatmap of path agreement across seeds\n",
    "\n",
    "import seaborn as sns  # if you prefer, you can use plain matplotlib, but seaborn is nice for heatmaps\n",
    "\n",
    "labels = [f\"seed={r['seed']}\" for r in results_multi_seed]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(agreement_matrix, annot=True, fmt=\".2f\",\n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            vmin=0.0, vmax=1.0, cmap=\"viridis\")\n",
    "plt.title(f\"Viterbi path agreement (K={K_target})\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278d684",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Look at means and variances of each state across seeds\n",
    "\n",
    "all_means = []\n",
    "all_vars = []\n",
    "\n",
    "for res in results_multi_seed:\n",
    "    params = res[\"params\"]\n",
    "    all_means.append(params[\"means\"])\n",
    "    all_vars.append(params[\"vars\"])\n",
    "\n",
    "all_means = np.array(all_means)  # shape: (n_runs, K)\n",
    "all_vars = np.array(all_vars)\n",
    "\n",
    "print(\"State means across seeds:\\n\", all_means)\n",
    "print(\"\\nState variances across seeds:\\n\", all_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df7003",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# boxplots for means and variances\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].boxplot(all_means, labels=[f\"state {k}\" for k in range(K_target)])\n",
    "axes[0].set_title(\"Means per state across seeds\")\n",
    "\n",
    "axes[1].boxplot(all_vars, labels=[f\"state {k}\" for k in range(K_target)])\n",
    "axes[1].set_title(\"Variances per state across seeds\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5074274a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Macro alignment: define some rough macro episodes and inspect regime usage\n",
    "\n",
    "# Choose one representative run for macro analysis\n",
    "best_res = results_multi_seed[0]   # you can also pick the best BIC one\n",
    "path = best_res[\"viterbi_path\"]\n",
    "\n",
    "df_macro = df.copy()\n",
    "df_macro[\"regime\"] = path  # 0..K-1\n",
    "\n",
    "# Example macro windows (you should tune these dates to real episodes)\n",
    "macro_windows = [\n",
    "    (\"Pre-crisis\",  \"2000-01-01\", \"2006-12-31\"),\n",
    "    (\"Crisis\",      \"2007-01-01\", \"2011-12-31\"),\n",
    "    (\"Post-crisis\", \"2012-01-01\", \"2019-12-31\"),\n",
    "    (\"Recent\",      \"2020-01-01\", \"2025-12-31\"),\n",
    "]\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for name, start, end in macro_windows:\n",
    "    mask = (df_macro[\"date\"] >= start) & (df_macro[\"date\"] <= end)\n",
    "    sub = df_macro[mask]\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "\n",
    "    counts = sub[\"regime\"].value_counts(normalize=True).sort_index()\n",
    "    row = {\"Period\": name, \"T\": len(sub)}\n",
    "    for k in range(K_target):\n",
    "        row[f\"regime_{k}_share\"] = counts.get(k, 0.0)\n",
    "    summary_rows.append(row)\n",
    "\n",
    "df_macro_summary = pd.DataFrame(summary_rows)\n",
    "df_macro_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53c487",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the returns with regime bands and mark macro windows for visual alignment\n",
    "\n",
    "state_colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "ax.plot(df_macro[\"date\"], df_macro[\"ret\"], label=\"log-return\")\n",
    "ax.set_ylabel(\"log-return\")\n",
    "ax.set_title(f\"HMM regimes (K={K_target}) with macro windows\")\n",
    "\n",
    "# regime bands\n",
    "for t in range(len(df_macro)):\n",
    "    ax.axvspan(\n",
    "        df_macro[\"date\"].iloc[t],\n",
    "        df_macro[\"date\"].iloc[t] + pd.Timedelta(\"1D\"),\n",
    "        color=state_colors[df_macro[\"regime\"].iloc[t]],\n",
    "        alpha=0.06,\n",
    "    )\n",
    "\n",
    "# vertical lines for macro window boundaries\n",
    "for name, start, end in macro_windows:\n",
    "    ax.axvline(pd.to_datetime(start), color=\"k\", linestyle=\"--\", alpha=0.4)\n",
    "    ax.axvline(pd.to_datetime(end), color=\"k\", linestyle=\"--\", alpha=0.2)\n",
    "    ax.text(pd.to_datetime(start), ax.get_ylim()[1],\n",
    "            name, rotation=90, verticalalignment=\"bottom\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a3f3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Construct a simple downturn proxy label\n",
    "\n",
    "df_down = df_macro.copy()\n",
    "\n",
    "# rolling 3-month cumulative return\n",
    "df_down[\"ret_3m\"] = df_down[\"ret\"].rolling(window=3).sum()\n",
    "\n",
    "# downturn label: 1 if 3-month sum < 0, else 0\n",
    "df_down[\"downturn_label\"] = (df_down[\"ret_3m\"] < 0).astype(int)\n",
    "\n",
    "df_down[[\"date\", \"ret\", \"ret_3m\", \"downturn_label\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d09383",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get smoothed posterior gamma_t(k) and build a \"downturn score\" -1\n",
    "\n",
    "# Use the same params as best_res\n",
    "params = best_res[\"params\"]\n",
    "\n",
    "# compute posteriors (gamma_t(k))\n",
    "log_alpha, loglik = forward_pass(observations, params)\n",
    "log_beta = backward_pass(observations, params)\n",
    "gamma = compute_posteriors(log_alpha, log_beta)   # shape (T, K)\n",
    "\n",
    "gamma[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3be92",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Pick the \"down-like\" regime as the state with smallest mean -2\n",
    "\n",
    "means = params[\"means\"]\n",
    "down_state = np.argmin(means)\n",
    "down_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f75b88",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Build a simple downturn score from smoothed posterior -3\n",
    "\n",
    "downturn_score = gamma[:, down_state]   # 1D array of probabilities\n",
    "\n",
    "df_down[\"downturn_score\"] = downturn_score\n",
    "df_down[[\"date\", \"downturn_label\", \"downturn_score\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea6ce1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ROC curve and AUC for the downturn label vs HMM downturn_score\n",
    "\n",
    "# Drop early rows where ret_3m is NaN\n",
    "mask_valid = ~df_down[\"ret_3m\"].isna()\n",
    "y_true = df_down.loc[mask_valid, \"downturn_label\"].values\n",
    "y_score = df_down.loc[mask_valid, \"downturn_score\"].values\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"random\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC for downturn detection using HMM regime probability\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC for downturn detection: {roc_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcf1df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Simple \"early warning\" check: shift the score by 1 period into the past\n",
    "\n",
    "lead = 1  # you can also try 2 or 3\n",
    "df_down[f\"downturn_score_lead{lead}\"] = df_down[\"downturn_score\"].shift(lead)\n",
    "\n",
    "mask_valid_lead = (~df_down[\"ret_3m\"].isna()) & (~df_down[f\"downturn_score_lead{lead}\"].isna())\n",
    "y_true_lead = df_down.loc[mask_valid_lead, \"downturn_label\"].values\n",
    "y_score_lead = df_down.loc[mask_valid_lead, f\"downturn_score_lead{lead}\"].values\n",
    "\n",
    "fpr_lead, tpr_lead, _ = roc_curve(y_true_lead, y_score_lead)\n",
    "roc_auc_lead = auc(fpr_lead, tpr_lead)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"Same-period (AUC={roc_auc:.3f})\")\n",
    "plt.plot(fpr_lead, tpr_lead, label=f\"Lead {lead} step (AUC={roc_auc_lead:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"random\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC comparison: same-period vs early warning\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
